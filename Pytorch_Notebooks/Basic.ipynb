{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x1 = t.randn(1, 1, 1024, 1024)\n",
    "x2 = t.randn(1, 1, 32, 32)\n",
    "\n",
    "print(x1.shape) \n",
    "\n",
    "diffY = x2.size()[2] - x1.size()[2]\n",
    "diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "x1 = F.pad(x1, [diffX // 2, diffX // 2, diffY // 2, diffY // 2])\n",
    "\n",
    "print(x1.ndim)\n",
    "\n",
    "print(t.cat([x2, x1], dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.rand(4, 10)\n",
    "label_A = logits.argmax(dim=1)\n",
    "print(logits.argmax(dim=1))\n",
    "\n",
    "pred = F.softmax(logits, dim=1) #torch.Size([4, 10])\n",
    "label_B = pred.argmax(dim=1) #torch.Size([4])\n",
    "print(label_B)\n",
    "\n",
    "correct = torch.eq(label_A, label_B)\n",
    "\n",
    "print(correct.sum().float().item()/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = t.Tensor(5,3)\n",
    "\n",
    "x = t.rand(5,3)\n",
    "#print(x.size())\n",
    "#print(x.size()[0],x.size(1))\n",
    "\n",
    "y = t.rand(5,3)\n",
    "result = t.Tensor(5,3)\n",
    "#print(x + y)\n",
    "t.add(x,y,out=result)\n",
    "#print(result)\n",
    "#print(y.add(x)) #不覆盖加法\n",
    "#print(y.add_(x)) #覆盖加法\n",
    "\n",
    "a = t.ones(5).numpy()\n",
    "#print(a)\n",
    "a = t.from_numpy(a)\n",
    "#print(a)\n",
    "\n",
    "######\n",
    "# Tensor\n",
    "a = t.Tensor(2, 3)\n",
    "print(a.size(), a.numel())\n",
    "# a.numel()：总元素个数\n",
    "a = t.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = t.Tensor(a.size())\n",
    "c = t.arange(1, 6, 2).tolist()\n",
    "#print(c)\n",
    "\n",
    "# 索引操作\n",
    "b = t.randn(3, 4)\n",
    "#print(b[0], b[0] > 0)\n",
    "#print(b[:, 0], b[:, 0] > 0)\n",
    "\n",
    "\n",
    "\n",
    "if t.cuda.is_available():\n",
    "    print(\"cuda is available now\")\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y\n",
    "else:\n",
    "    print(\"cuda is not available\")\n",
    "\n",
    "## Pytorch 中 view 的用法\n",
    "'''\n",
    "把原先tensor中的数据按照行优先的顺序排成一个一维的数据\n",
    "（这里应该是因为要求地址是连续存储的），然后按照参数组合成其他维度的tensor。\n",
    "比如说是不管你原先的数据是[[[1,2,3],[4,5,6]]]还是[1,2,3,4,5,6]，\n",
    "因为它们排成一维向量都是6个元素，所以只要view函数的参数一致，得到的结果都是一样的。\n",
    "'''\n",
    "a=t.Tensor([[[1,2,3],[4,5,6]]])\n",
    "b=t.Tensor([[[[[1,2,3,4,5,6]]]]])\n",
    "\n",
    "print(a.view(1, 1, 1, -1))\n",
    "#print(b.view(1,6))\n",
    "\n",
    "#print(a.view(1, -1)) # \"-1\" 为自适应\n",
    "#print(b.view(-1, 2,))\n",
    "\n",
    "########\n",
    "m = nn.Linear(20, 20)\n",
    "#print(m)\n",
    "input = t.randn(128, 20)\n",
    "output = m(input)\n",
    "#print(output.size())\n",
    "#print(output,input)\n",
    "#torch.Size([128, 30])\n",
    "\n",
    "\n",
    "########\n",
    "#Pytorch L\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af1ef7c2422b98f0aa0eb8c8ba68e52bc5e98ee3d10707544ebeb5411de3d7d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
