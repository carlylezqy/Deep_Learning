{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.2938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.1877, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.0903, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(1.0012, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.9201, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.8464, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7796, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.7192, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "tensor(0.6647, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Matmul(torch.nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.randn(shape, requires_grad=True))\n",
    "        self.b = torch.nn.Parameter(torch.randn(shape, requires_grad=True))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        z = torch.matmul(x, self.w) + self.b\n",
    "        #z = x @ self.w + self.b\n",
    "        return z\n",
    "\n",
    "x = torch.ones((5, 5))\n",
    "y = torch.zeros((5, 5))\n",
    "\n",
    "model = Matmul((5, 5))\n",
    "\n",
    "lr = 1\n",
    "\n",
    "for i in range(10):\n",
    "    pred_y = model(x)\n",
    "\n",
    "    loss = torch.nn.functional.binary_cross_entropy_with_logits(pred_y, y)\n",
    "    loss.backward()\n",
    "\n",
    "    model.w.data = model.w.data - lr * model.b.grad\n",
    "    model.b.data = model.b.data - lr * model.b.grad\n",
    "\n",
    "    #直替换w, b的值，会导致w，b脱离计算图，无法进行反向传播\n",
    "    #改变其值只需要改变其data的值即可\n",
    "\n",
    "    model.w.grad.zero_()\n",
    "    model.b.grad.zero_()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0097 0.0 -0.10\n",
      "0.0072 0.0 -0.08\n",
      "0.0176 1.0 0.87\n",
      "0.0009 1.0 0.97\n",
      "0.0000 1.0 0.99\n",
      "0.0050 0.0 -0.07\n",
      "0.0047 0.0 -0.07\n",
      "0.0045 0.0 -0.07\n",
      "0.0041 0.0 -0.06\n",
      "0.0000 1.0 1.01\n",
      "0.0037 0.0 -0.06\n",
      "0.0000 1.0 1.01\n",
      "0.0034 0.0 -0.06\n",
      "0.0000 1.0 1.00\n",
      "0.0000 1.0 1.00\n",
      "0.0000 1.0 1.00\n",
      "0.0032 0.0 -0.06\n",
      "0.0030 0.0 -0.06\n",
      "0.0028 0.0 -0.05\n",
      "0.0027 0.0 -0.05\n",
      "0.0026 0.0 -0.05\n",
      "0.0000 1.0 1.00\n",
      "0.0025 0.0 -0.05\n",
      "0.0000 1.0 1.00\n",
      "0.0023 0.0 -0.05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, (5, 5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "x1 =  torch.ones((1, 5, 5)).to(torch.float32)\n",
    "x2 = torch.zeros((1, 5, 5)).to(torch.float32)\n",
    "\n",
    "y1 =  torch.ones((1, 1, 1)).to(torch.float32)\n",
    "y2 = torch.zeros((1, 1, 1)).to(torch.float32)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "model.train()\n",
    "for i in range(50):\n",
    "    model.zero_grad()\n",
    "    x, y = (x1, y1) if random.randint(0,1) else (x2, y2)\n",
    "    pred_y = model(x)\n",
    "    loss = criterion(pred_y, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    if i % 5 == 0:\n",
    "        print(f\"%.4f %.1f %.2f\" % (loss.item(), y.mean(), pred_y.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0101 0.8066 [0.00 0.10] [1.00 0.10]\n",
      "0.0077 0.3236 [0.00 0.09] [1.00 0.43]\n",
      "0.0000 0.0002 [1.00 1.00] [0.00 0.02]\n",
      "0.0010 0.0414 [0.00 0.03] [1.00 0.80]\n",
      "0.0000 0.0000 [1.00 1.00] [0.00 0.00]\n",
      "0.0002 0.0088 [0.00 0.01] [1.00 0.91]\n",
      "0.0001 0.0037 [0.00 0.01] [1.00 0.94]\n",
      "0.0000 0.0000 [1.00 1.00] [0.00 0.00]\n",
      "0.0000 0.0006 [0.00 0.00] [1.00 0.97]\n",
      "0.0000 0.0000 [1.00 1.00] [0.00 0.00]\n"
     ]
    }
   ],
   "source": [
    "#Need to optimization\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda1 = torch.device('cuda:1')  \n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.root1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 1, (5, 5)),\n",
    "        )\n",
    "        self.root2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 1, (5, 5)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        root1 = self.root1(x).to(cuda0)\n",
    "        root2 = self.root2(x).to(cuda1)\n",
    "        fusion = root1 + root2.to(cuda0)\n",
    "\n",
    "        # root1  --> cuda0\n",
    "        # root2  --> cuda1\n",
    "        # fusion --> cuda0\n",
    "\n",
    "        return [root1, root2, fusion]\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = Model()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "model.train()\n",
    "for i in range(500):\n",
    "    model.zero_grad()\n",
    "\n",
    "    if random.randint(0, 10) % 2 == 0:\n",
    "        x  = torch.ones((1, 5, 5)).to(torch.float32)\n",
    "        y1 = torch.ones((1, 1, 1)).to(torch.float32).to(cuda0)\n",
    "        y2 =torch.zeros((1, 1, 1)).to(torch.float32).to(cuda1)\n",
    "    else:\n",
    "        x =  torch.zeros((1, 5, 5)).to(torch.float32)\n",
    "        y1 = torch.zeros((1, 1, 1)).to(torch.float32).to(cuda0)\n",
    "        y2 =  torch.ones((1, 1, 1)).to(torch.float32).to(cuda1)\n",
    "\n",
    "    r1, r2, r12 = model(x)\n",
    "    loss1 = criterion(r1, y1)\n",
    "    loss2 = criterion(r2, y2).to(cuda0)\n",
    "    loss =  loss1 + loss2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(f\"%.4f %.4f [%.2f %.2f] [%.2f %.2f]\" % (loss1.item(), loss2.item(), y1.mean(), r1.mean(), y2.mean(), r2.mean()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "225b954b04ff71a4e7f3ebd4997bd511ef4b566637ad554feaa6403c6bffc147"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
