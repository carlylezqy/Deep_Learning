{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from mmengine.model import BaseModel\n",
    "\n",
    "class MMResNet50(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "    def forward(self, imgs, labels, mode):\n",
    "        x = self.resnet(imgs)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, labels)}\n",
    "        elif mode == 'predict':\n",
    "            return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "norm_cfg = dict(mean=[0.491, 0.482, 0.447], std=[0.202, 0.199, 0.201])\n",
    "train_transformer = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**norm_cfg)\n",
    "])\n",
    "\n",
    "vaild_transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**norm_cfg)\n",
    "])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    batch_size=128, shuffle=True,\n",
    "    dataset=torchvision.datasets.CIFAR10(\n",
    "        '/home/akiyo/nfs/zhang/dataset',\n",
    "        train=True, download=True, transform=train_transformer\n",
    "))\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    batch_size=128, shuffle=False,\n",
    "    dataset=torchvision.datasets.CIFAR10(\n",
    "        '/home/akiyo/nfs/zhang/dataset',\n",
    "        train=False, download=True, transform=vaild_transformer\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.evaluator import BaseMetric\n",
    "\n",
    "class Accuracy(BaseMetric):\n",
    "    def process(self, data_batch, data_samples):\n",
    "        score, gt = data_samples\n",
    "        # save the middle result of a batch to `self.results`\n",
    "        self.results.append({\n",
    "            'batch_size': len(gt),\n",
    "            'correct': (score.argmax(dim=1) == gt).sum().cpu(),\n",
    "        })\n",
    "\n",
    "    def compute_metrics(self, results):\n",
    "        total_correct = sum(item['correct'] for item in results)\n",
    "        total_size = sum(item['batch_size'] for item in results)\n",
    "        # return the dict containing the eval results\n",
    "        # the key is the name of the metric name\n",
    "        return dict(accuracy=100 * total_correct / total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/akiyo/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:02<00:00, 35.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/15 17:25:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.15 (default, Nov 24 2022, 15:19:38) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 146479247\n",
      "    GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.4, V11.4.152\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
      "    PyTorch: 1.11.0+cu113\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.12.0+cu113\n",
      "    OpenCV: 4.6.0\n",
      "    MMEngine: 0.3.2\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: None\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "12/15 17:25:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Config:\n",
      "\n",
      "\n",
      "12/15 17:25:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Result has been saved to /home/akiyo/sandbox/work_dirs/quick_start/modules_statistic_results.json\n",
      "12/15 17:25:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "12/15 17:25:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/zhang/github/mmengine/mmengine/runner/loops.py:56: UserWarning: Dataset CIFAR10 has no metainfo. ``dataset_meta`` in visualizer will be None.\n",
      "  warnings.warn(\n",
      "/share/zhang/github/mmengine/mmengine/evaluator/metric.py:47: UserWarning: The prefix is not set in metric class Accuracy.\n",
      "  warnings.warn('The prefix is not set in metric class '\n",
      "/share/zhang/github/mmengine/mmengine/runner/loops.py:335: UserWarning: Dataset CIFAR10 has no metainfo. ``dataset_meta`` in evaluator, metric and visualizer will be None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/15 17:25:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Checkpoints will be saved to /home/akiyo/sandbox/work_dirs/quick_start.\n",
      "12/15 17:25:28 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 10/391]  lr: 1.0000e-03  eta: 0:03:52  time: 0.1197  data_time: 0.0564  memory: 1159  loss: 10.1526\n",
      "12/15 17:25:29 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 20/391]  lr: 1.0000e-03  eta: 0:03:49  time: 0.1170  data_time: 0.0651  memory: 1159  loss: 5.6264\n",
      "12/15 17:25:30 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 30/391]  lr: 1.0000e-03  eta: 0:03:49  time: 0.1212  data_time: 0.0669  memory: 1159  loss: 3.5788\n",
      "12/15 17:25:31 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 40/391]  lr: 1.0000e-03  eta: 0:03:49  time: 0.1214  data_time: 0.0704  memory: 1159  loss: 2.4943\n",
      "12/15 17:25:32 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 50/391]  lr: 1.0000e-03  eta: 0:03:44  time: 0.1093  data_time: 0.0636  memory: 1159  loss: 2.2143\n",
      "12/15 17:25:33 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 60/391]  lr: 1.0000e-03  eta: 0:03:40  time: 0.1089  data_time: 0.0628  memory: 1159  loss: 1.8330\n",
      "12/15 17:25:34 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 70/391]  lr: 1.0000e-03  eta: 0:03:36  time: 0.1060  data_time: 0.0606  memory: 1159  loss: 1.7323\n",
      "12/15 17:25:36 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 80/391]  lr: 1.0000e-03  eta: 0:03:34  time: 0.1101  data_time: 0.0632  memory: 1159  loss: 1.6446\n",
      "12/15 17:25:37 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][ 90/391]  lr: 1.0000e-03  eta: 0:03:32  time: 0.1107  data_time: 0.0655  memory: 1159  loss: 1.5412\n",
      "12/15 17:25:38 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][100/391]  lr: 1.0000e-03  eta: 0:03:29  time: 0.1076  data_time: 0.0606  memory: 1159  loss: 1.4541\n",
      "12/15 17:25:39 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][110/391]  lr: 1.0000e-03  eta: 0:03:27  time: 0.1046  data_time: 0.0577  memory: 1159  loss: 1.3867\n",
      "12/15 17:25:40 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][120/391]  lr: 1.0000e-03  eta: 0:03:25  time: 0.1057  data_time: 0.0609  memory: 1159  loss: 1.3316\n",
      "12/15 17:25:41 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][130/391]  lr: 1.0000e-03  eta: 0:03:24  time: 0.1135  data_time: 0.0623  memory: 1159  loss: 1.3084\n",
      "12/15 17:25:42 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][140/391]  lr: 1.0000e-03  eta: 0:03:23  time: 0.1145  data_time: 0.0615  memory: 1159  loss: 1.3053\n",
      "12/15 17:25:43 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][150/391]  lr: 1.0000e-03  eta: 0:03:22  time: 0.1161  data_time: 0.0628  memory: 1159  loss: 1.2439\n",
      "12/15 17:25:44 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][160/391]  lr: 1.0000e-03  eta: 0:03:22  time: 0.1177  data_time: 0.0632  memory: 1159  loss: 1.2207\n",
      "12/15 17:25:46 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][170/391]  lr: 1.0000e-03  eta: 0:03:20  time: 0.1097  data_time: 0.0597  memory: 1159  loss: 1.2364\n",
      "12/15 17:25:47 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][180/391]  lr: 1.0000e-03  eta: 0:03:20  time: 0.1173  data_time: 0.0658  memory: 1159  loss: 1.2383\n",
      "12/15 17:25:48 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][190/391]  lr: 1.0000e-03  eta: 0:03:19  time: 0.1199  data_time: 0.0660  memory: 1159  loss: 1.1682\n",
      "12/15 17:25:49 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][200/391]  lr: 1.0000e-03  eta: 0:03:18  time: 0.1160  data_time: 0.0642  memory: 1159  loss: 1.1565\n",
      "12/15 17:25:50 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][210/391]  lr: 1.0000e-03  eta: 0:03:17  time: 0.1123  data_time: 0.0615  memory: 1159  loss: 1.1578\n",
      "12/15 17:25:51 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][220/391]  lr: 1.0000e-03  eta: 0:03:16  time: 0.1137  data_time: 0.0623  memory: 1159  loss: 1.1220\n",
      "12/15 17:25:52 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][230/391]  lr: 1.0000e-03  eta: 0:03:15  time: 0.1166  data_time: 0.0631  memory: 1159  loss: 1.1088\n",
      "12/15 17:25:54 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][240/391]  lr: 1.0000e-03  eta: 0:03:15  time: 0.1232  data_time: 0.0719  memory: 1159  loss: 1.0943\n",
      "12/15 17:25:55 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][250/391]  lr: 1.0000e-03  eta: 0:03:14  time: 0.1185  data_time: 0.0646  memory: 1159  loss: 1.0751\n",
      "12/15 17:25:56 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][260/391]  lr: 1.0000e-03  eta: 0:03:13  time: 0.1169  data_time: 0.0656  memory: 1159  loss: 1.0313\n",
      "12/15 17:25:57 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][270/391]  lr: 1.0000e-03  eta: 0:03:12  time: 0.1212  data_time: 0.0666  memory: 1159  loss: 0.9952\n",
      "12/15 17:25:58 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][280/391]  lr: 1.0000e-03  eta: 0:03:11  time: 0.1143  data_time: 0.0616  memory: 1159  loss: 0.9824\n",
      "12/15 17:26:00 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][290/391]  lr: 1.0000e-03  eta: 0:03:10  time: 0.1183  data_time: 0.0651  memory: 1159  loss: 0.9921\n",
      "12/15 17:26:01 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][300/391]  lr: 1.0000e-03  eta: 0:03:09  time: 0.1044  data_time: 0.0609  memory: 1159  loss: 1.0527\n",
      "12/15 17:26:02 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][310/391]  lr: 1.0000e-03  eta: 0:03:07  time: 0.1101  data_time: 0.0621  memory: 1159  loss: 0.9312\n",
      "12/15 17:26:03 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][320/391]  lr: 1.0000e-03  eta: 0:03:06  time: 0.1096  data_time: 0.0642  memory: 1159  loss: 0.9955\n",
      "12/15 17:26:04 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][330/391]  lr: 1.0000e-03  eta: 0:03:05  time: 0.1162  data_time: 0.0720  memory: 1159  loss: 1.0665\n",
      "12/15 17:26:05 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][340/391]  lr: 1.0000e-03  eta: 0:03:04  time: 0.1136  data_time: 0.0685  memory: 1159  loss: 0.9702\n",
      "12/15 17:26:06 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][350/391]  lr: 1.0000e-03  eta: 0:03:02  time: 0.1072  data_time: 0.0647  memory: 1159  loss: 0.9402\n",
      "12/15 17:26:07 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][360/391]  lr: 1.0000e-03  eta: 0:03:01  time: 0.1066  data_time: 0.0620  memory: 1159  loss: 0.9439\n",
      "12/15 17:26:08 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][370/391]  lr: 1.0000e-03  eta: 0:02:59  time: 0.1053  data_time: 0.0607  memory: 1159  loss: 0.9431\n",
      "12/15 17:26:09 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][380/391]  lr: 1.0000e-03  eta: 0:02:58  time: 0.1149  data_time: 0.0705  memory: 1159  loss: 0.9711\n",
      "12/15 17:26:11 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [1][390/391]  lr: 1.0000e-03  eta: 0:02:57  time: 0.1079  data_time: 0.0624  memory: 1159  loss: 0.8356\n",
      "12/15 17:26:11 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Exp name: 20221215_172525\n",
      "12/15 17:26:11 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "12/15 17:26:11 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "12/15 17:26:15 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][10/79]    eta: 0:00:03  time: 0.0449  data_time: 0.0304  memory: 1076  \n",
      "12/15 17:26:15 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][20/79]    eta: 0:00:02  time: 0.0436  data_time: 0.0295  memory: 916  \n",
      "12/15 17:26:15 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][30/79]    eta: 0:00:02  time: 0.0433  data_time: 0.0292  memory: 916  \n",
      "12/15 17:26:16 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][40/79]    eta: 0:00:01  time: 0.0436  data_time: 0.0294  memory: 916  \n",
      "12/15 17:26:16 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][50/79]    eta: 0:00:01  time: 0.0438  data_time: 0.0297  memory: 916  \n",
      "12/15 17:26:17 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][60/79]    eta: 0:00:00  time: 0.0452  data_time: 0.0309  memory: 916  \n",
      "12/15 17:26:17 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][70/79]    eta: 0:00:00  time: 0.0440  data_time: 0.0298  memory: 916  \n",
      "12/15 17:26:18 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [1][79/79]  accuracy: 70.4100\n",
      "12/15 17:26:19 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 10/391]  lr: 1.0000e-03  eta: 0:02:56  time: 0.1109  data_time: 0.0582  memory: 1159  loss: 0.8509\n",
      "12/15 17:26:20 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 20/391]  lr: 1.0000e-03  eta: 0:02:55  time: 0.1178  data_time: 0.0651  memory: 1159  loss: 0.8743\n",
      "12/15 17:26:21 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 30/391]  lr: 1.0000e-03  eta: 0:02:54  time: 0.1167  data_time: 0.0640  memory: 1159  loss: 0.8525\n",
      "12/15 17:26:22 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 40/391]  lr: 1.0000e-03  eta: 0:02:53  time: 0.1135  data_time: 0.0621  memory: 1159  loss: 0.8198\n",
      "12/15 17:26:23 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 50/391]  lr: 1.0000e-03  eta: 0:02:52  time: 0.1091  data_time: 0.0641  memory: 1159  loss: 0.8097\n",
      "12/15 17:26:24 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 60/391]  lr: 1.0000e-03  eta: 0:02:50  time: 0.1090  data_time: 0.0631  memory: 1159  loss: 0.8211\n",
      "12/15 17:26:26 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 70/391]  lr: 1.0000e-03  eta: 0:02:49  time: 0.1125  data_time: 0.0666  memory: 1159  loss: 0.8322\n",
      "12/15 17:26:27 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 80/391]  lr: 1.0000e-03  eta: 0:02:48  time: 0.1185  data_time: 0.0668  memory: 1159  loss: 0.8002\n",
      "12/15 17:26:28 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][ 90/391]  lr: 1.0000e-03  eta: 0:02:47  time: 0.1186  data_time: 0.0687  memory: 1159  loss: 0.7927\n",
      "12/15 17:26:29 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][100/391]  lr: 1.0000e-03  eta: 0:02:46  time: 0.1203  data_time: 0.0674  memory: 1159  loss: 0.8616\n",
      "12/15 17:26:30 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][110/391]  lr: 1.0000e-03  eta: 0:02:45  time: 0.1145  data_time: 0.0624  memory: 1159  loss: 0.8138\n",
      "12/15 17:26:31 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][120/391]  lr: 1.0000e-03  eta: 0:02:44  time: 0.1132  data_time: 0.0631  memory: 1159  loss: 0.8187\n",
      "12/15 17:26:33 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][130/391]  lr: 1.0000e-03  eta: 0:02:43  time: 0.1159  data_time: 0.0658  memory: 1159  loss: 0.8696\n",
      "12/15 17:26:34 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][140/391]  lr: 1.0000e-03  eta: 0:02:42  time: 0.1111  data_time: 0.0636  memory: 1159  loss: 0.8475\n",
      "12/15 17:26:35 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][150/391]  lr: 1.0000e-03  eta: 0:02:40  time: 0.1125  data_time: 0.0626  memory: 1159  loss: 0.8352\n",
      "12/15 17:26:36 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][160/391]  lr: 1.0000e-03  eta: 0:02:39  time: 0.1126  data_time: 0.0646  memory: 1159  loss: 0.7814\n",
      "12/15 17:26:37 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][170/391]  lr: 1.0000e-03  eta: 0:02:38  time: 0.1141  data_time: 0.0621  memory: 1159  loss: 0.7988\n",
      "12/15 17:26:38 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][180/391]  lr: 1.0000e-03  eta: 0:02:37  time: 0.1168  data_time: 0.0665  memory: 1159  loss: 0.7644\n",
      "12/15 17:26:39 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][190/391]  lr: 1.0000e-03  eta: 0:02:36  time: 0.1113  data_time: 0.0635  memory: 1159  loss: 0.7801\n",
      "12/15 17:26:41 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][200/391]  lr: 1.0000e-03  eta: 0:02:35  time: 0.1151  data_time: 0.0632  memory: 1159  loss: 0.7728\n",
      "12/15 17:26:42 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][210/391]  lr: 1.0000e-03  eta: 0:02:34  time: 0.1107  data_time: 0.0599  memory: 1159  loss: 0.8080\n",
      "12/15 17:26:43 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][220/391]  lr: 1.0000e-03  eta: 0:02:32  time: 0.1163  data_time: 0.0638  memory: 1159  loss: 0.8017\n",
      "12/15 17:26:44 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][230/391]  lr: 1.0000e-03  eta: 0:02:31  time: 0.1199  data_time: 0.0661  memory: 1159  loss: 0.7580\n",
      "12/15 17:26:45 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][240/391]  lr: 1.0000e-03  eta: 0:02:30  time: 0.1136  data_time: 0.0615  memory: 1159  loss: 0.7747\n",
      "12/15 17:26:46 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][250/391]  lr: 1.0000e-03  eta: 0:02:29  time: 0.1154  data_time: 0.0670  memory: 1159  loss: 0.7414\n",
      "12/15 17:26:47 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][260/391]  lr: 1.0000e-03  eta: 0:02:28  time: 0.1163  data_time: 0.0640  memory: 1159  loss: 0.7178\n",
      "12/15 17:26:49 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][270/391]  lr: 1.0000e-03  eta: 0:02:27  time: 0.1133  data_time: 0.0625  memory: 1159  loss: 0.8225\n",
      "12/15 17:26:50 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][280/391]  lr: 1.0000e-03  eta: 0:02:26  time: 0.1104  data_time: 0.0600  memory: 1159  loss: 0.7387\n",
      "12/15 17:26:51 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][290/391]  lr: 1.0000e-03  eta: 0:02:25  time: 0.1185  data_time: 0.0647  memory: 1159  loss: 0.7493\n",
      "12/15 17:26:52 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][300/391]  lr: 1.0000e-03  eta: 0:02:24  time: 0.1125  data_time: 0.0634  memory: 1159  loss: 0.7275\n",
      "12/15 17:26:53 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][310/391]  lr: 1.0000e-03  eta: 0:02:22  time: 0.1139  data_time: 0.0632  memory: 1159  loss: 0.7121\n",
      "12/15 17:26:54 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][320/391]  lr: 1.0000e-03  eta: 0:02:21  time: 0.1083  data_time: 0.0638  memory: 1159  loss: 0.7191\n",
      "12/15 17:26:55 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][330/391]  lr: 1.0000e-03  eta: 0:02:20  time: 0.1120  data_time: 0.0611  memory: 1159  loss: 0.7775\n",
      "12/15 17:26:57 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][340/391]  lr: 1.0000e-03  eta: 0:02:19  time: 0.1211  data_time: 0.0660  memory: 1159  loss: 0.7026\n",
      "12/15 17:26:58 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][350/391]  lr: 1.0000e-03  eta: 0:02:18  time: 0.1059  data_time: 0.0600  memory: 1159  loss: 0.7414\n",
      "12/15 17:26:59 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][360/391]  lr: 1.0000e-03  eta: 0:02:16  time: 0.0965  data_time: 0.0611  memory: 1159  loss: 0.7691\n",
      "12/15 17:26:59 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][370/391]  lr: 1.0000e-03  eta: 0:02:15  time: 0.0911  data_time: 0.0555  memory: 1159  loss: 0.7621\n",
      "12/15 17:27:01 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][380/391]  lr: 1.0000e-03  eta: 0:02:14  time: 0.1200  data_time: 0.0656  memory: 1159  loss: 0.6736\n",
      "12/15 17:27:02 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [2][390/391]  lr: 1.0000e-03  eta: 0:02:13  time: 0.1179  data_time: 0.0644  memory: 1159  loss: 0.6726\n",
      "12/15 17:27:02 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Exp name: 20221215_172525\n",
      "12/15 17:27:02 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "12/15 17:27:02 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "12/15 17:27:05 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][10/79]    eta: 0:00:03  time: 0.0441  data_time: 0.0297  memory: 1076  \n",
      "12/15 17:27:06 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][20/79]    eta: 0:00:02  time: 0.0447  data_time: 0.0301  memory: 916  \n",
      "12/15 17:27:06 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][30/79]    eta: 0:00:02  time: 0.0441  data_time: 0.0299  memory: 916  \n",
      "12/15 17:27:07 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][40/79]    eta: 0:00:01  time: 0.0438  data_time: 0.0297  memory: 916  \n",
      "12/15 17:27:07 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][50/79]    eta: 0:00:01  time: 0.0438  data_time: 0.0296  memory: 916  \n",
      "12/15 17:27:08 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][60/79]    eta: 0:00:00  time: 0.0433  data_time: 0.0292  memory: 916  \n",
      "12/15 17:27:08 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][70/79]    eta: 0:00:00  time: 0.0438  data_time: 0.0297  memory: 916  \n",
      "12/15 17:27:08 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(val) [2][79/79]  accuracy: 76.2400\n",
      "12/15 17:27:10 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 10/391]  lr: 1.0000e-03  eta: 0:02:11  time: 0.1079  data_time: 0.0583  memory: 1159  loss: 0.7383\n",
      "12/15 17:27:11 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 20/391]  lr: 1.0000e-03  eta: 0:02:10  time: 0.1084  data_time: 0.0629  memory: 1159  loss: 0.6634\n",
      "12/15 17:27:12 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 30/391]  lr: 1.0000e-03  eta: 0:02:09  time: 0.1080  data_time: 0.0614  memory: 1159  loss: 0.6930\n",
      "12/15 17:27:13 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 40/391]  lr: 1.0000e-03  eta: 0:02:08  time: 0.1052  data_time: 0.0598  memory: 1159  loss: 0.7084\n",
      "12/15 17:27:14 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 50/391]  lr: 1.0000e-03  eta: 0:02:07  time: 0.1108  data_time: 0.0628  memory: 1159  loss: 0.6411\n",
      "12/15 17:27:15 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 60/391]  lr: 1.0000e-03  eta: 0:02:05  time: 0.0965  data_time: 0.0609  memory: 1159  loss: 0.6779\n",
      "12/15 17:27:16 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 70/391]  lr: 1.0000e-03  eta: 0:02:04  time: 0.0936  data_time: 0.0581  memory: 1159  loss: 0.6768\n",
      "12/15 17:27:17 - mmengine - \u001b[4m\u001b[37mINFO\u001b[0m - Epoch(train) [3][ 80/391]  lr: 1.0000e-03  eta: 0:02:03  time: 0.0938  data_time: 0.0586  memory: 1159  loss: 0.6967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmmengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunner\u001b[39;00m \u001b[39mimport\u001b[39;00m Runner\n\u001b[1;32m      4\u001b[0m runner \u001b[39m=\u001b[39m Runner(\n\u001b[1;32m      5\u001b[0m     \u001b[39m# the model used for training and validation.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39m# Needs to meet specific interface requirements\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     val_evaluator\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mAccuracy),\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m runner\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/share/zhang/github/mmengine/mmengine/runner/runner.py:1684\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1678\u001b[0m \u001b[39m# Initiate inner count of `optim_wrapper`.\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_wrapper\u001b[39m.\u001b[39minitialize_count_status(\n\u001b[1;32m   1680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1681\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_loop\u001b[39m.\u001b[39miter,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_loop\u001b[39m.\u001b[39mmax_iters)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 1684\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loop\u001b[39m.\u001b[39;49mrun()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_run\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1686\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/share/zhang/github/mmengine/mmengine/runner/loops.py:90\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mbefore_train\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_epochs:\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_epoch()\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decide_current_val_interval()\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mval_loop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_begin\n\u001b[1;32m     95\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m/share/zhang/github/mmengine/mmengine/runner/loops.py:106\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m idx, data_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_iter(idx, data_batch)\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\u001b[39m'\u001b[39m\u001b[39mafter_train_epoch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/share/zhang/github/mmengine/mmengine/runner/loops.py:122\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_iter\u001b[0;34m(self, idx, data_batch)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\n\u001b[1;32m    118\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbefore_train_iter\u001b[39m\u001b[39m'\u001b[39m, batch_idx\u001b[39m=\u001b[39midx, data_batch\u001b[39m=\u001b[39mdata_batch)\n\u001b[1;32m    119\u001b[0m \u001b[39m# Enable gradient accumulation mode and avoid unnecessary gradient\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39m# synchronization during gradient accumulation process.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# outputs should be a dict of loss.\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunner\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_step(\n\u001b[1;32m    123\u001b[0m     data_batch, optim_wrapper\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunner\u001b[39m.\u001b[39;49moptim_wrapper)\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunner\u001b[39m.\u001b[39mcall_hook(\n\u001b[1;32m    126\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mafter_train_iter\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    127\u001b[0m     batch_idx\u001b[39m=\u001b[39midx,\n\u001b[1;32m    128\u001b[0m     data_batch\u001b[39m=\u001b[39mdata_batch,\n\u001b[1;32m    129\u001b[0m     outputs\u001b[39m=\u001b[39moutputs)\n\u001b[1;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/share/zhang/github/mmengine/mmengine/model/base_model/base_model.py:114\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[0;34m(self, data, optim_wrapper)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m optim_wrapper\u001b[39m.\u001b[39moptim_context(\u001b[39mself\u001b[39m):\n\u001b[1;32m    113\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_preprocessor(data, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 114\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_forward(data, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    115\u001b[0m parsed_losses, log_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_losses(losses)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    116\u001b[0m optim_wrapper\u001b[39m.\u001b[39mupdate_params(parsed_losses)\n",
      "File \u001b[0;32m/share/zhang/github/mmengine/mmengine/model/base_model/base_model.py:316\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[0;34m(self, data, mode)\u001b[0m\n\u001b[1;32m    314\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata, mode\u001b[39m=\u001b[39mmode)\n\u001b[1;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 316\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mdata, mode\u001b[39m=\u001b[39;49mmode)\n\u001b[1;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput of `data_preprocessor` should be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    319\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlist, tuple or dict, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mMMResNet50.forward\u001b[0;34m(self, imgs, labels, mode)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, imgs, labels, mode):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet(imgs)\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     13\u001b[0m         \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: F\u001b[39m.\u001b[39mcross_entropy(x, labels)}\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torchvision/models/resnet.py:283\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 283\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torchvision/models/resnet.py:271\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    268\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    269\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    272\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torchvision/models/resnet.py:145\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m identity \u001b[39m=\u001b[39m x\n\u001b[1;32m    144\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m--> 145\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(out)\n\u001b[1;32m    146\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m    148\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:147\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m     exponential_average_factor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    146\u001b[0m     \u001b[39m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_batches_tracked \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_batches_tracked\u001b[39m.\u001b[39madd_(\u001b[39m1\u001b[39m)  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# use cumulative moving average\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/swint/lib/python3.8/site-packages/torch/nn/modules/module.py:1178\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[39mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_buffers\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m-> 1178\u001b[0m     _buffers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m_buffers\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m   1179\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _buffers:\n\u001b[1;32m   1180\u001b[0m         \u001b[39mreturn\u001b[39;00m _buffers[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "runner = Runner(\n",
    "    # the model used for training and validation.\n",
    "    # Needs to meet specific interface requirements\n",
    "    model=MMResNet50(),\n",
    "    # working directory which saves training logs and weight files\n",
    "    work_dir='/home/akiyo/sandbox/work_dirs/quick_start',\n",
    "    # train dataloader needs to meet the PyTorch data loader protocol\n",
    "    train_dataloader=train_dataloader,\n",
    "    # optimize wrapper for optimization with additional features like\n",
    "    # AMP, gradtient accumulation, etc\n",
    "    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),\n",
    "    # trainging coinfs for specifying training epoches, verification intervals, etc\n",
    "    train_cfg=dict(by_epoch=True, max_epochs=5, val_interval=1),\n",
    "    # validation dataloaer also needs to meet the PyTorch data loader protocol\n",
    "    val_dataloader=val_dataloader,\n",
    "    # validation configs for specifying additional parameters required for validation\n",
    "    val_cfg=dict(),\n",
    "    # validation evaluator. The default one is used here\n",
    "    val_evaluator=dict(type=Accuracy),\n",
    ")\n",
    "\n",
    "runner.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af1ef7c2422b98f0aa0eb8c8ba68e52bc5e98ee3d10707544ebeb5411de3d7d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
